{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "import Levenshtein as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('all/train.npy', encoding='bytes')\n",
    "Y_train = np.load('all/character/train_labels.npy')\n",
    "X_dev = np.load('all/dev.npy', encoding='bytes')\n",
    "Y_dev = np.load('all/character/dev_labels.npy')\n",
    "vocab_map = np.load('all/character/vocab.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __getitem__(self,i):\n",
    "        if self.labels != None:\n",
    "            return torch.tensor(self.data[i]), torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(self.data[i])\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "def collate(seq_list):\n",
    "    \"\"\"\n",
    "    return: a batch sorted by decreasing order of length of sequences\n",
    "    inputs: (L_padded, batch_size, 40)\n",
    "    targets: list of targets\n",
    "    \"\"\"\n",
    "    inputs,targets = zip(*seq_list)\n",
    "    lens = [seq.shape[0] for seq in inputs]\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    inputs = [inputs[i] for i in seq_order]\n",
    "    targets = [targets[i] for i in seq_order]\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setting: listener layer:2, hidden: 128/direction;  speller layer: 1, hidden: 256\n",
    "#### after that: listener layer:3, hidden: 256; speller: *, hidden: 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListenerModel(nn.Module):\n",
    "    def __init__(self, hidden_size=256, key_size=128, value_size=128, embed_size=40, nlayers=4): # todo: nlayers=4\n",
    "        \n",
    "        super(ListenerModel, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # todo: add conv layer\n",
    "        \n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTM(embed_size if i==0 else hidden_size*4, hidden_size, num_layers=1, bidirectional=True) for i in range(nlayers)\n",
    "        ])\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*2, key_size)\n",
    "        self.fc2 = nn.Linear(hidden_size*2, value_size)\n",
    "        self.weight_init()\n",
    "    \n",
    "    def weight_init(self):\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        key: (L, N, key_size)\n",
    "        value: (L, N, value_size)\n",
    "        hidden: hidden state of last layer at t = L, (h_n, c_n); h_n, (num_direction,bs,hidden)\n",
    "        \"\"\"\n",
    "        batch_size = len(inputs)\n",
    "        lens = [len(s) for s in inputs] # actual lens of all sequences (already sorted) \n",
    "        # todo: add conv layer\n",
    "        padded_input = rnn.pad_sequence(inputs).to(DEVICE)  # (L_padded, N, 40)      \n",
    "        out_packed = rnn.pack_padded_sequence(padded_input, lens)\n",
    "        for (i,l) in enumerate(self.rnns):\n",
    "            out_packed, hidden = l(out_packed)\n",
    "            out_padded, lens = rnn.pad_packed_sequence(out_packed)\n",
    "            seq_len, batch_size, dim = out_padded.size()\n",
    "            \n",
    "            if i < self.nlayers-1:\n",
    "                if seq_len % 2 == 1:\n",
    "                    out_padded = out_padded[:seq_len-1,:,:]\n",
    "                    seq_len, batch_size, dim = out_padded.size()\n",
    "                out_padded = out_padded.permute(1,0,2).contiguous().view(batch_size, seq_len // 2, dim*2).permute(1,0,2)\n",
    "                lens = [i // 2 for i in lens]\n",
    "                out_packed = rnn.pack_padded_sequence(out_padded, lens)   # (L_padded, N, hidden_size)\n",
    "        \n",
    "        key = self.fc1(out_padded)\n",
    "        value = self.fc2(out_padded)\n",
    "        return key, value, hidden, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell_(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell_, self).__init__()\n",
    "        self.lstmcell = nn.LSTMCell(input_size, hidden_size, bias)\n",
    "        self.h_0 = nn.Parameter(torch.randn(1,hidden_size).to(DEVICE))\n",
    "        self.c_0 = nn.Parameter(torch.randn(1,hidden_size).to(DEVICE))\n",
    "        self.parameters = nn.ParameterList([self.h_0,self.c_0])\n",
    "    \n",
    "    def forward(self, inputs, t, hidden_states):\n",
    "        if t == 0:\n",
    "            hidden_states = (self.h_0.expand(inputs.size(0),-1), self.c_0.expand(inputs.size(0),-1))\n",
    "        hidden_state = self.lstmcell(inputs, hidden_states)\n",
    "        return hidden_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=512, embed_size=256, key_size=128, nlayers=2):\n",
    "        super(SpellerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.nlayers = nlayers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # projection os state s\n",
    "        self.fc1 = nn.Linear(hidden_size, key_size)\n",
    "        \n",
    "        self.rnns = nn.ModuleList([\n",
    "            LSTMCell_(embed_size+key_size if layer==0 else hidden_size, hidden_size) for layer in range(nlayers)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.scoring = nn.Linear(hidden_size+key_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, audio_lens, key, value, teacher=False, hidden_init=None):\n",
    "        \"\"\"\n",
    "        inputs: (L2_padded, batch_size), L2_padded = padded transcript length \n",
    "        audio_lens: list of actual length of audio sequences\n",
    "        key: (L_padded, bs, key_size)\n",
    "        value: (L_padded, bs, value_size)\n",
    "        query_init: (bs, hidden)\n",
    "        context: (batch_size, context_size)\n",
    "        outs: (L2_padded, bs, vocab_size)\n",
    "        hiddens: a list of (h_n, c_n), n = L2_padded\n",
    "        \"\"\"\n",
    "        L2_padded = inputs.size(0)     \n",
    "        L_padded, N, _ = value.size()\n",
    "        key = key.permute(1, 2, 0)        # -> (N,key_size, L_padded)\n",
    "        value = value.permute(1, 0, 2)   # -> (N, L, value_size)\n",
    "        \n",
    "        embed = self.embed(inputs)   # (L2_padded, batch_size, embed_size)\n",
    "        hiddens = [None] * self.nlayers\n",
    "        outs = []\n",
    "        \n",
    "        # create mask for padded audio utterance\n",
    "        mask = torch.ones(N, 1, L_padded).to(DEVICE) \n",
    "        for i in range(len(audio_lens)):\n",
    "            mask[i,:,audio_lens[i]:] = 0\n",
    "            \n",
    "        # initial query\n",
    "        s = self.rnns[-1].h_0.expand(N,-1)  # (N, key_size)\n",
    "        query = self.fc1(s).unsqueeze(1)   # (N, 1, key_size)\n",
    "        \n",
    "        for i in range(embed.size(0)): \n",
    "            if (i!=0) and teacher:\n",
    "                x = outs[-1]  # (N, vocab_size)\n",
    "                x = F.softmax(x, dim=1) # (N, vocab_size)\n",
    "                x = torch.argmax(x, dim=1) # (N, 1)\n",
    "                y = self.embed(x)  # (N, embed_size)\n",
    "            else:\n",
    "                y = embed[i]    #(N, embed_size)\n",
    "            # create context\n",
    "            energy = torch.bmm(query, key)      #(N, 1, L_padded)\n",
    "            attention = F.softmax(energy, 2)    #(N, 1, L_padded)\n",
    "            attention = torch.mul(mask, attention)  \n",
    "            # normalize\n",
    "            attention = attention / (torch.sum(attention, dim=1)+1e-12).unsqueeze(1)\n",
    "            context = torch.bmm(attention, value).squeeze(1)  #(N, value_size)\n",
    "            \n",
    "            inp = torch.cat((y, context), 1)    # (N, embed+value)\n",
    "            h = inp\n",
    "            for (l, rnn) in enumerate(self.rnns):\n",
    "                h1, c1 = rnn(h, i, hiddens[l])\n",
    "                hiddens[l] = (h1,c1)\n",
    "                h = h1            \n",
    "            outs.append(self.scoring(torch.cat((h,context),1)))\n",
    "            query = self.fc1(h).unsqueeze(1)\n",
    "        \n",
    "        outs = torch.stack(outs, dim=0)  #(L2_padded, N, vocab_size)\n",
    "        return outs, hiddens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTrainer:\n",
    "    def __init__ (self, listener, speller, train_loader, val_loader, max_epochs=1, run_id='exp'):\n",
    "        self.listener = listener.to(DEVICE)\n",
    "        self.speller = speller.to(DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        self.optimizer1 = torch.optim.Adam(self.speller.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        self.optimizer2 = torch.optim.Adam(self.speller.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduce='false').to(DEVICE)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer2, factor = 0.1, patience = 3, mode = 'min')\n",
    "    \n",
    "    def train(self):\n",
    "        self.listener.train()\n",
    "        self.speller.train()\n",
    "        self.epochs += 1\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_num, (inputs, targets) in enumerate(self.train_loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "       \n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "   \n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "    \n",
    "    def train_batch(self, inputs, targets):\n",
    "        # listener generates key/value, s_{-1},_ = hidden\n",
    "        key, value, hidden, seq_lens = self.listener(inputs)\n",
    "        \n",
    "        trans_lens = [len(t) for t in targets]\n",
    "        targets = rnn.pad_sequence(targets, padding_value=0).to(DEVICE) # (L2_padded, bs) \n",
    "        inp = targets[:-1,:]\n",
    "        mask = torch.ones(inp.size()).to(DEVICE)  # (L2_padded, bs)\n",
    "        for i in range(len(trans_lens)):\n",
    "            mask[trans_lens[i]-1:,i] = 0\n",
    "        \n",
    "        teacher=False\n",
    "#         if (np.random.randn() > 0.9):\n",
    "#             teacher = True\n",
    "        outs,_ = self.speller(inputs=inp, audio_lens=seq_lens, key=key, value=value, teacher=teacher)\n",
    "        loss = self.criterion(outs.view(-1, outs.size(2)), targets[1:,:].view(-1))    \n",
    "        loss = torch.sum(torch.mul(mask.view(-1), loss))   \n",
    "        \n",
    "        self.optimizer1.zero_grad()\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer1.step()\n",
    "        self.optimizer2.step()\n",
    "        \n",
    "        return loss.item() / len(trans_lens)\n",
    "    \n",
    "    def validate(self):\n",
    "        self.listener.eval()\n",
    "        self.speller.eval()\n",
    "        val_loss = 0\n",
    "        ls = 0\n",
    "        dev_len = len(self.val_loader.dataset)\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (inputs, targets) in enumerate(self.val_loader):\n",
    "                key, value, _, seq_lens = self.listener(inputs)\n",
    "\n",
    "                for t in targets:\n",
    "                    trues.append(t.cpu().numpy())\n",
    "                    \n",
    "                trans_lens = [len(t) for t in targets]\n",
    "                targets = rnn.pad_sequence(targets, padding_value=0).to(DEVICE) # (L2_padded, bs) \n",
    "                inp = targets[:-1,:]\n",
    "                mask = torch.ones(inp.size()).to(DEVICE)  # (L2_padded, bs)\n",
    "                for i in range(len(trans_lens)):\n",
    "                    mask[trans_lens[i]-1:,i] = 0\n",
    "                outs,_ = self.speller(inputs=inp, audio_lens=seq_lens, key=key, value=value)\n",
    "                loss = self.criterion(outs.view(-1, outs.size(2)), targets[1:,:].view(-1))    \n",
    "                loss = torch.sum(torch.mul(mask.view(-1), loss))   \n",
    "                val_loss += (loss.item() / len(trans_lens)\n",
    "                \n",
    "                probs = F.softmax(outs.permute(1,0,2), dim=2) # (N, L2_padded, vocab_size)\n",
    "                pred = torch.argmax(probs, dim=2) # (N, L2_padded)\n",
    "                for p in pred:\n",
    "                    preds.append(p.cpu().numpy())\n",
    "                \n",
    "                probs = F.softmax(outs.permute(1,0,2), dim=2) # (N, L2_padded-1, vocab_size)\n",
    "                pred = torch.argmax(probs, dim=2) # (N, L2_padded)\n",
    "                for p in pred:\n",
    "                    preds.append(p.cpu().numpy())\n",
    "                \n",
    "            for i in range(dev_len):\n",
    "                pred_i = preds[i]\n",
    "                true_i = trues[i][1:-1]   # trues include <sos> and <eos>\n",
    "                if 0 in pred_i:\n",
    "                    pred_i = pred_i[:pred_i.tolist().index(0)]\n",
    "\n",
    "                pred = \"\".join(vocab_map[o] for o in pred_i)\n",
    "                true = \"\".join(vocab_map[l] for l in true_i)\n",
    "                if i % 100 == 0:\n",
    "                    print('pred:', pred)\n",
    "                    print('true:', true)\n",
    "\n",
    "                ls += L.distance(pred, true)\n",
    "                \n",
    "            return val_loss / (batch_num + 1), ls / dev_len\n",
    "    \n",
    "    def save_model(self):\n",
    "        path1 = os.path.join('attention', self.run_id, 'listener-{}.pkl'.format(self.epochs))\n",
    "        path2 = os.path.join('attention', self.run_id, 'speller-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.listener.state_dict()}, path1)\n",
    "        torch.save({'state_dict': self.speller.state_dict()}, path2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CharDataset(X_train[:80], Y_train[:80])\n",
    "devset = CharDataset(X_dev[:40], Y_dev[:40])\n",
    "train_loader = DataLoader(trainset, shuffle=True, batch_size=BATCH_SIZE, collate_fn = collate)\n",
    "dev_loader = DataLoader(devset, batch_size=BATCH_SIZE, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./attention/1542253115\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./attention'):\n",
    "    os.mkdir('./attention')\n",
    "os.mkdir('./attention/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./attention/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = ListenerModel()\n",
    "speller = SpellerModel(vocab_size=len(vocab_map))\n",
    "# l = torch.load('attention/1542253115/listener-5.pkl')\n",
    "# listener.load_state_dict(l['state_dict'])\n",
    "# s = torch.load('attention/1542253115/speller-5.pkl')\n",
    "# speller.load_state_dict(s['state_dict'])\n",
    "# checkpoint = torch.load('experiments/1542219735/speller-13.pkl')\n",
    "# speller.load_state_dict(checkpoint['state_dict'])\n",
    "trainer = AttentionTrainer(listener=listener, speller=speller, \n",
    "                           train_loader=train_loader, val_loader=dev_loader, \n",
    "                           max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/5]   Loss: 5128.7017\n",
      "Val loss: 4821.005181206598, Val Levenshtein distance: 73.00904159132007\n",
      "Saving model for epoch 1, with Levenshtein distance: 73.00904159132007\n",
      "Time elapsed:  00:05:02\n",
      "[TRAIN]  Epoch [2/5]   Loss: 4888.3238\n",
      "Val loss: 4871.135932074652, Val Levenshtein distance: 72.63200723327306\n",
      "Saving model for epoch 2, with Levenshtein distance: 72.63200723327306\n",
      "Time elapsed:  00:05:03\n",
      "[TRAIN]  Epoch [3/5]   Loss: 4773.7500\n",
      "Val loss: 4957.209526909723, Val Levenshtein distance: 72.91862567811935\n",
      "Time elapsed:  00:05:02\n",
      "[TRAIN]  Epoch [4/5]   Loss: 4689.9039\n",
      "Val loss: 4992.079644097223, Val Levenshtein distance: 72.72784810126582\n",
      "Time elapsed:  00:05:02\n",
      "[TRAIN]  Epoch [5/5]   Loss: 4551.6683\n",
      "Val loss: 5035.129069010417, Val Levenshtein distance: 73.17721518987342\n",
      "Time elapsed:  00:05:02\n"
     ]
    }
   ],
   "source": [
    "best_dist = 1e30\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "\n",
    "    val_loss, dist = trainer.validate()\n",
    "    print('Val loss: {}, Val Levenshtein distance: {}'.format(val_loss, dist))\n",
    "    if dist < best_dist:\n",
    "        best_dist = dist\n",
    "        print(\"Saving model for epoch {}, with Levenshtein distance: {}\".format(epoch+1, best_dist))\n",
    "        trainer.save_model()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Time elapsed: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tnteady comma the srirmiceutical rarafacturers  rrsociateon aas betuisted tnsirmy sive yyphen yay yxerndivn oo the nompirtsoeriod\n",
      "true: already comma the pharmaceutical manufacturers' association has requested a forty five hyphen day extension to the comment period comma saying it needs more time for consideration period\n",
      "pred: tonsectiout punne ioa petg  af teare  ttetsirsng ttvm aonital mands \n",
      "true: connecticut joins the ranks of states sponsoring seed capital funds\n",
      "pred: tor the first time in years the republicans also captured both houses of congress\n",
      "true: at least one person was killed and more than two hundred and fifty people were injured in the blasts\n",
      "pred: the satoestaam  hhadenional beaelopment oonhnicues uich as larus oroupsas oneeewe fhth thngumer  \n",
      "true: the south carolina educational radio network has won national broadcasting awards\n",
      "pred: tovl  arocesiakits ahre on tart on ers d to rvlow thph at estmrs ah tryticupate dn the parket sa manit ng the r soases ff t y dooen yey aeriod\n",
      "true: the state government will invest five million dollars to launch the connecticut seed venture fund and is asking private industry to invest about an equal amount period\n",
      "pred: the population lives by herding goats and sheep or by trading\n",
      "true: they don't have the luxury of taking the easy way out period double-quote\n",
      "pred: tt is tne of the earliest agricultural villages yet discovered in southwest asia\n",
      "true: he never obtained a secure academic position or permanent employment\n",
      "pred: tho narrow gauge railroads from china enter the city from the northeast and northwest\n",
      "true: a tanker is a ship designed to carry large volumes of oil or other liquid cargo\n",
      "pred: tnhersspntrities airms wncludeng toerrson lehman brothers iamding  in orporated car se srayneng tpxplar ptretegics tu e oasture iopital sts wpid\n",
      "true: she also refused cash offers to relocate including one for six hundred fifty thousand dollars\n",
      "pred: te sremosed a lne hillion dollar prderal oesaoin ng premrem tor torcoace  trcmers tnd tolls frr telal rmtertrisesien y ieere mommanies teuld bitraox reeacs eor tatatlisheng prans  \n",
      "true: in southern california comma a skilled programmer typically earns thirty five dollars an hour comma one thousand comma four hundred dollars for a forty hyphen hour week period\n",
      "pred: tuth petroleum and natural gas deposits are scattered through eastern ohio\n",
      "true: mr. jacob's efforts have sparked an uproar in sun city period\n",
      "pred: they ban also show how the shape and size of continents and oceans have changed over time\n",
      "true: the market has to do that period\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5035.129069010417, 73.17721518987342)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
