{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "import Levenshtein as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('all/train.npy', encoding='bytes')\n",
    "Y_train = np.load('all/character/train_labels.npy')\n",
    "X_dev = np.load('all/dev.npy', encoding='bytes')\n",
    "Y_dev = np.load('all/character/dev_labels.npy')\n",
    "vocab_map = np.load('all/character/vocab.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __getitem__(self,i):\n",
    "        if self.labels != None:\n",
    "            return torch.tensor(self.data[i]), torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor(self.data[i])\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "def collate(seq_list):\n",
    "    \"\"\"\n",
    "    return: a batch sorted by decreasing order of length of sequences\n",
    "    inputs: (L_padded, batch_size, 40)\n",
    "    targets: list of targets\n",
    "    \"\"\"\n",
    "    inputs,targets = zip(*seq_list)\n",
    "    lens = [seq.shape[0] for seq in inputs]\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    inputs = [inputs[i] for i in seq_order]\n",
    "    targets = [targets[i] for i in seq_order]\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setting: listener layer:2, hidden: 128/direction;  speller layer: 1, hidden: 256\n",
    "#### after that: listener layer:3, hidden: 256; speller: *, hidden: 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListenerModel(nn.Module):\n",
    "    def __init__(self, hidden_size=256, key_size=128, value_size=128, embed_size=40, nlayers=3):\n",
    "        super(ListenerModel, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # todo: add conv layer\n",
    "        \n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTM(embed_size if i==0 else hidden_size*4, hidden_size, num_layers=1, bidirectional=True) for i in range(nlayers)\n",
    "        ])\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*4, key_size)\n",
    "        self.fc2 = nn.Linear(key_size, value_size)\n",
    "        self.weight_init()\n",
    "    \n",
    "    def weight_init(self):\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.fc1.bias.data.fill_(0.01)\n",
    "        self.fc2.bias.data.fill_(0.01)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        key: (L, N, key_size)\n",
    "        value: (L, N, value_size)\n",
    "        hidden: hidden state of last layer at t = L, (h_n, c_n); h_n, (num_direction,bs,hidden)\n",
    "        \"\"\"\n",
    "        batch_size = len(inputs)\n",
    "        lens = [len(s) for s in inputs] # actual lens of all sequences (already sorted) \n",
    "        # todo: add conv layer\n",
    "        padded_input = rnn.pad_sequence(inputs).to(DEVICE)  # (L_padded, N, 40)\n",
    "        out_packed = rnn.pack_padded_sequence(padded_input, lens)\n",
    "        for l in self.rnns:\n",
    "            out_packed, hidden = l(out_packed)\n",
    "            out_padded, _ = rnn.pad_packed_sequence(out_packed)\n",
    "            seq_len, batch_size, dim = out_padded.size()\n",
    "            if seq_len % 2 == 1:\n",
    "                out_padded = out_padded[:seq_len-1,:,:]\n",
    "                seq_len, batch_size, dim = out_padded.size()\n",
    "                lens[:] = [i-1 for i in lens]\n",
    "            out_padded = out_padded.permute(1,0,2).contiguous().view(batch_size, seq_len // 2, dim*2).permute(1,0,2)\n",
    "            lens[:] = [i // 2 for i in lens]\n",
    "            out_packed = rnn.pack_padded_sequence(out_padded, lens)   # (L_padded, N, hidden_size)\n",
    "        \n",
    "        key = self.fc1(out_padded)\n",
    "        value = self.fc2(key)\n",
    "        return key, value, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=512, embed_size=256, key_size=128, nlayers=2):\n",
    "        super(SpellerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # projection os state s\n",
    "        self.fc1 = nn.Linear(hidden_size, key_size)\n",
    "        \n",
    "        # 1st layer, input: cat(y_{i-1},context_{i-1}); h_0: s_{i-1}\n",
    "        self.rnns = nn.ModuleList([\n",
    "            nn.LSTMCell(embed_size+key_size if layer==0 else hidden_size, hidden_size) for layer in range(nlayers)\n",
    "        ])\n",
    "        \n",
    "        self.scoring = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, key, value, hidden_init=None):\n",
    "        \"\"\"\n",
    "        inputs: (L2_padded, batch_size), L2_padded = padded transcript length \n",
    "        key: (L_padded, bs, key_size)\n",
    "        value: (L_padded, bs, value_size)\n",
    "        query_init: (bs, hidden)\n",
    "        context: (batch_size, context_size)\n",
    "        outs: (L2_padded, bs, vocab_size)\n",
    "        hiddens: a list of (h_n, c_n), n = L2_padded\n",
    "        \"\"\"\n",
    "        key = key.permute(1, 2, 0)\n",
    "        value = value.permute(1, 0, 2)\n",
    "#         print(inputs.size())\n",
    "        \n",
    "        embed = self.embed(inputs)   # (L2_padded, batch_size, embed_size)\n",
    "        hiddens = [None] * self.nlayers\n",
    "        for (i,h) in enumerate(hidden_init):\n",
    "            hiddens[i] = h\n",
    "        outs = []\n",
    "        \n",
    "        for y in embed: #(N, embed_size)\n",
    "            # create context\n",
    "            s,_ = hiddens[0]\n",
    "            query = self.fc1(s).unsqueeze(1)   # (N, 1, key_size)\n",
    "            # create context\n",
    "            energy = torch.bmm(query, key)  #(N, 1, L_padded)\n",
    "            attention = F.log_softmax(energy, 2)\n",
    "            context = torch.bmm(attention, value).squeeze()  #(N, value_size)\n",
    "            \n",
    "            inp = torch.cat((y, context), 1)    # (N, embed+value)\n",
    "            h = inp\n",
    "            for (l, rnn) in enumerate(self.rnns):\n",
    "                h1, c1 = rnn(h, hiddens[l])\n",
    "                hiddens[l] = (h1,c1)\n",
    "                h = h1\n",
    "\n",
    "            outs.append(self.scoring(h))\n",
    "        \n",
    "        outs = torch.stack(outs, dim=0)  #(L2_padded, N, vocab_size)\n",
    "        return outs, hiddens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionTrainer:\n",
    "    def __init__ (self, listener, speller, train_loader, val_loader, max_epochs=1, run_id='exp'):\n",
    "        self.listener = listener.to(DEVICE)\n",
    "        self.speller = speller.to(DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        self.optimizer1 = torch.optim.Adam(self.speller.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "        self.optimizer2 = torch.optim.Adam(self.speller.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduce='false').to(DEVICE)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer2, factor = 0.1, patience = 3, mode = 'min')\n",
    "    \n",
    "    def train(self):\n",
    "        self.listener.train()\n",
    "        self.speller.train()\n",
    "        self.epochs += 1\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_num, (inputs, targets) in enumerate(self.train_loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "       \n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "   \n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "    \n",
    "    def train_batch(self, inputs, targets):\n",
    "        # listener generates key/value, s_{-1},_ = hidden\n",
    "        key, value, hidden = self.listener(inputs)\n",
    "        a,bs,hs = hidden[0].size()\n",
    "        h_1 = hidden[0].permute(1,0,2).contiguous().view(-1,a*hs)\n",
    "        c_1 = hidden[1].permute(1,0,2).contiguous().view(-1,a*hs)\n",
    "        hidden_init = [(h_1, c_1)]\n",
    "        \n",
    "        lens = [len(t) for t in targets]\n",
    "        targets = rnn.pad_sequence(targets, padding_value=0).to(DEVICE) # (L2_padded, bs) \n",
    "        inp = targets[:-1,:]\n",
    "        mask = torch.ones(inp.size()).to(DEVICE)  # (L2_padded, bs)\n",
    "        for i in range(len(lens)):\n",
    "            mask[lens[i]-1:,i] = 0\n",
    "        outs,_ = self.speller(inputs=inp, key=key, value=value, hidden_init=hidden_init)\n",
    "        loss = self.criterion(outs.view(-1, outs.size(2)), targets[1:,:].view(-1))    \n",
    "        loss = torch.sum(torch.mul(mask.view(-1), loss))   \n",
    "        \n",
    "        self.optimizer1.zero_grad()\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer1.step()\n",
    "        self.optimizer2.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self):\n",
    "        self.listener.eval()\n",
    "        self.speller.eval()\n",
    "        val_loss = 0\n",
    "        ls = 0\n",
    "        dev_len = len(self.val_loader.dataset)\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (inputs, targets) in enumerate(self.val_loader):\n",
    "                key, value, hidden = self.listener(inputs)\n",
    "                a,bs,hs = hidden[0].size()\n",
    "                h_1 = hidden[0].permute(1,0,2).contiguous().view(-1,a*hs)\n",
    "                c_1 = hidden[1].permute(1,0,2).contiguous().view(-1,a*hs)\n",
    "                hidden_init = [(h_1, c_1)]\n",
    "                \n",
    "                for t in targets:\n",
    "                    trues.append(t.cpu().numpy())\n",
    "                    \n",
    "                lens = [len(t) for t in targets]\n",
    "                targets = rnn.pad_sequence(targets, padding_value=0).to(DEVICE) # (L2_padded, bs) \n",
    "                inp = targets[:-1,:]\n",
    "                mask = torch.ones(inp.size()).to(DEVICE)  # (L2_padded, bs)\n",
    "                for i in range(len(lens)):\n",
    "                    mask[lens[i]-1:,i] = 0\n",
    "                outs,_ = self.speller(inputs=inp, key=key, value=value, hidden_init=hidden_init)\n",
    "                loss = self.criterion(outs.view(-1, outs.size(2)), targets[1:,:].view(-1))    \n",
    "                loss = torch.sum(torch.mul(mask.view(-1), loss))   \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = F.softmax(outs.permute(1,0,2), dim=2) # (N, L2_padded, vocab_size)\n",
    "#                 print('probs0: ',probs[0])\n",
    "#                 print(torch.max(probs[0],dim=1))\n",
    "                pred = torch.argmax(probs, dim=2) # (N, L2_padded)\n",
    "#                 print(pred.size())\n",
    "                for p in pred:\n",
    "                    preds.append(p.cpu().numpy())\n",
    "                \n",
    "                probs = F.softmax(outs.permute(1,0,2), dim=2) # (N, L2_padded-1, vocab_size)\n",
    "                pred = torch.argmax(probs, dim=2) # (N, L2_padded)\n",
    "                for p in pred:\n",
    "                    preds.append(p.cpu().numpy())\n",
    "                \n",
    "            for i in range(dev_len):\n",
    "                pred_i = preds[i]\n",
    "                true_i = trues[i][1:-1]   # trues include <sos> and <eos>\n",
    "                if 0 in pred_i:\n",
    "                    pred_i = pred_i[:pred_i.tolist().index(0)]\n",
    "\n",
    "                pred = \"\".join(vocab_map[o] for o in pred_i)\n",
    "                true = \"\".join(vocab_map[l] for l in true_i)\n",
    "                if i % 500 == 0:\n",
    "                    print('pred:', pred)\n",
    "                    print('true:', true)\n",
    "\n",
    "                ls += L.distance(pred, true)\n",
    "                \n",
    "            return val_loss / (batch_num + 1), ls / dev_len\n",
    "    \n",
    "    def save_model(self):\n",
    "        path1 = os.path.join('attention', self.run_id, 'listener-{}.pkl'.format(self.epochs))\n",
    "        path2 = os.path.join('attention', self.run_id, 'speller-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.listener.state_dict()}, path1)\n",
    "        torch.save({'state_dict': self.speller.state_dict()}, path2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CharDataset(X_train[6400:12800], Y_train[6400:12800])\n",
    "devset = CharDataset(X_dev, Y_dev)\n",
    "train_loader = DataLoader(trainset, shuffle=True, batch_size=BATCH_SIZE, collate_fn = collate)\n",
    "dev_loader = DataLoader(devset, shuffle=True, batch_size=BATCH_SIZE, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./attention/1542229876\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./attention'):\n",
    "    os.mkdir('./attention')\n",
    "os.mkdir('./attention/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./attention/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = ListenerModel()\n",
    "speller = SpellerModel(vocab_size=len(vocab_map))\n",
    "l = torch.load('attention/1542229337/listener-3.pkl')\n",
    "listener.load_state_dict(l['state_dict'])\n",
    "s = torch.load('attention/1542229337/speller-3.pkl')\n",
    "speller.load_state_dict(s['state_dict'])\n",
    "# checkpoint = torch.load('experiments/1542219735/speller-13.pkl')\n",
    "# speller.load_state_dict(checkpoint['state_dict'])\n",
    "trainer = AttentionTrainer(listener=listener, speller=speller, \n",
    "                           train_loader=train_loader, val_loader=dev_loader, \n",
    "                           max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/5]   Loss: 31497.1464\n",
      "pred: thet tor n n nen aen nn teteae tn  ne  then aen e  thr aon   torn  aon ao  ne tn e   e   aoreantnnieneaoreontneaen tn  aoreontoen n  toreontntnne tor n  aoreontn ao  n   aorn ar an toenen  toren   aor nn \n",
      "true: this paradoxical notion simply assumes that higher tax rates would not reduce investment comma output comma employment comma profits comma equity values comma or related sources of private savings period\n",
      "pred: the tereeaeneeae tn   ahrteen ne to   een  \n",
      "true: washington apparently got the word period\n",
      "pred: th e aoreenteen ae   aeen  ahen ahre tor   nn \n",
      "true: he's shaking up the management of french companies\n",
      "Val loss: 23508.386328125, Val Levenshtein distance: 79.29204339963833\n",
      "Saving model for epoch 1, with Levenshtein distance: 79.29204339963833\n",
      "Time elapsed:  00:01:28\n",
      "[TRAIN]  Epoch [2/5]   Loss: 30121.9456\n",
      "pred: tn tn  toe n   che tomee   an then tere  ne  toe ne tomeentere  ne  tn  ene co e te ee  to  n  nd aoe e to   \n",
      "true: as one breasts the current of this sometimes creamy comma sometimes awkward self hyphen regarding style left-parentheses it's obviously catching right-paren comma one inevitably thinks of death by drowning period\n",
      "pred: tn aet  s ao n tornt an  ee e tnd ahe tene ten aooeer\n",
      "true: cohen brothers won't talk about the standoff period\n",
      "pred: toteeaeee n aern an aer  n anteonte torto eeaer  aoe tn\n",
      "true: they currently are interested in acquiring kroger company period\n",
      "Val loss: 23163.85703125, Val Levenshtein distance: 77.33363471971067\n",
      "Saving model for epoch 2, with Levenshtein distance: 77.33363471971067\n",
      "Time elapsed:  00:01:28\n",
      "[TRAIN]  Epoch [3/5]   Loss: 28827.2198\n",
      "pred: tuut nn  torend tornl  surd  the     tnde e teoe n ne   oeo      e to      torlte ton n  an the teon  tn  tor r s te e   aomme ternee \n",
      "true: -lations period double-quote there's a very significant difference between policy making on the state and federal levels comma double-quote says john shannon comma executive director of the advisory commission on intergovernmental relations period\n",
      "pred: tn       oese  aonl ae tnd  n o  aer e   n \n",
      "true: richmond county is both period\n",
      "pred: the  ae the n  an aenh an e  tom  neon   ah mn  toete n o \n",
      "true: the population lives by herding goats and sheep or by trading\n",
      "Val loss: 21544.937109375, Val Levenshtein distance: 77.34177215189874\n",
      "Time elapsed:  00:01:28\n",
      "[TRAIN]  Epoch [4/5]   Loss: 27053.9590\n",
      "pred: tu.t on  aeriod toune  tuo e the   t tnde e tooeen oen  aoo e    e te ie   terlta taneog tn the tion  tnd tor   n tare i aomma teune  \n",
      "true: -lations period double-quote there's a very significant difference between policy making on the state and federal levels comma double-quote says john shannon comma executive director of the advisory commission on intergovernmental relations period\n",
      "pred: tetee t aeou  og tarn\n",
      "true: the study measures stock picking ability only\n",
      "pred: tut i e nneng aoann andetn teomeng aene tet  nas  ohet and \n",
      "true: during the following years he tried unsuccessfully to get it into production\n",
      "Val loss: 20339.2763671875, Val Levenshtein distance: 73.29023508137432\n",
      "Saving model for epoch 4, with Levenshtein distance: 73.29023508137432\n",
      "Time elapsed:  00:01:28\n",
      "[TRAIN]  Epoch [5/5]   Loss: 25546.1664\n",
      "pred: thes trret n nel ton nn toleee an  re  ahet tenh   ahr aeser aorrl ton tepent tn enteen  tomma snr eritomma snpeon e   oomma seomeci aomma snprte tenle  aomma snetepini  torreertaf teoneti tereng  aeriod\n",
      "true: this paradoxical notion simply assumes that higher tax rates would not reduce investment comma output comma employment comma profits comma equity values comma or related sources of private savings period\n",
      "pred: tetee s aeorteog torn\n",
      "true: it hadn't been going anywhere and the pace has picked up somewhat\n",
      "pred: tom   sn aor   af t eeone tot   tf the seark af the sertenine\n",
      "true: then he treats it with agree conditioner to add fragrance\n",
      "Val loss: 18992.89365234375, Val Levenshtein distance: 75.05877034358048\n",
      "Time elapsed:  00:01:28\n"
     ]
    }
   ],
   "source": [
    "best_dist = 1e30\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "\n",
    "    val_loss, dist = trainer.validate()\n",
    "    print('Val loss: {}, Val Levenshtein distance: {}'.format(val_loss, dist))\n",
    "    if dist < best_dist:\n",
    "        best_dist = dist\n",
    "        print(\"Saving model for epoch {}, with Levenshtein distance: {}\".format(epoch+1, best_dist))\n",
    "        trainer.save_model()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Time elapsed: ', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
